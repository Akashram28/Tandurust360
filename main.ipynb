{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Dropout, Input, Flatten, Bidirectional, Permute, multiply)\n",
    "\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, time_steps):\n",
    "    \"\"\"\n",
    "    Attention layer for deep neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    # Attention weights\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    \n",
    "    # Attention vector\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Luong's multiplicative score\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(HIDDEN_UNITS=256, sequence_length=30, num_input_values=33*4, num_classes=3):\n",
    "    \"\"\"\n",
    "    Function used to build the deep neural network model on startup\n",
    "\n",
    "    Args:\n",
    "        HIDDEN_UNITS (int, optional): Number of hidden units for each neural network hidden layer. Defaults to 256.\n",
    "        sequence_length (int, optional): Input sequence length (i.e., number of frames). Defaults to 30.\n",
    "        num_input_values (_type_, optional): Input size of the neural network model. Defaults to 33*4 (i.e., number of keypoints x number of metrics).\n",
    "        num_classes (int, optional): Number of classification categories (i.e., model output size). Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        keras model: neural network with pre-trained weights\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = Input(shape=(sequence_length, num_input_values))\n",
    "    # Bi-LSTM\n",
    "    lstm_out = Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True))(inputs)\n",
    "    # Attention\n",
    "    attention_mul = attention_block(lstm_out, sequence_length)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    # Fully Connected Layer\n",
    "    x = Dense(2*HIDDEN_UNITS, activation='relu')(attention_mul)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # Output\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    # Bring it all together\n",
    "    model = Model(inputs=[inputs], outputs=x)\n",
    "\n",
    "    ## Load Model Weights\n",
    "    load_dir = \"./models/LSTM_Attention.h5\"  \n",
    "    model.load_weights(load_dir)\n",
    "    \n",
    "    return model\n",
    "\n",
    "HIDDEN_UNITS = 256\n",
    "model = build_model(HIDDEN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose # Pre-trained pose estimation model from Google Mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils # Supported Mediapipe visualization tools\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) # mediapipe pose model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoProcessor:\n",
    "    def __init__(self):\n",
    "        # Parameters\n",
    "        self.actions = np.array(['curl', 'press', 'squat'])\n",
    "        self.sequence_length = 30\n",
    "        self.colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        # Detection variables\n",
    "        self.sequence = []\n",
    "        self.current_action = ''\n",
    "\n",
    "        # Rep counter logic variables\n",
    "        self.curl_counter = 0\n",
    "        self.press_counter = 0\n",
    "        self.squat_counter = 0\n",
    "        self.curl_stage = None\n",
    "        self.press_stage = None\n",
    "        self.squat_stage = None\n",
    "      \n",
    "    def draw_landmarks(self, image, results):\n",
    "        \"\"\"\n",
    "        This function draws keypoints and landmarks detected by the human pose estimation model\n",
    "        \n",
    "        \"\"\"\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                    )\n",
    "        return\n",
    "    \n",
    "    def extract_keypoints(self, results):\n",
    "        \"\"\"\n",
    "        Processes and organizes the keypoints detected from the pose estimation model \n",
    "        to be used as inputs for the exercise decoder models\n",
    "        \n",
    "        \"\"\"\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "        return pose\n",
    "    \n",
    "    def calculate_angle(self, a,b,c):\n",
    "        \"\"\"\n",
    "        Computes 3D joint angle inferred by 3 keypoints and their relative positions to one another\n",
    "        \n",
    "        \"\"\"\n",
    "        a = np.array(a) # First\n",
    "        b = np.array(b) # Mid\n",
    "        c = np.array(c) # End\n",
    "        \n",
    "        radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "        angle = np.abs(radians*180.0/np.pi)\n",
    "        \n",
    "        if angle > 180.0:\n",
    "            angle = 360-angle\n",
    "            \n",
    "        return angle \n",
    "\n",
    "    def get_coordinates(self, landmarks, mp_pose, side, joint):\n",
    "        \"\"\"\n",
    "        Retrieves x and y coordinates of a particular keypoint from the pose estimation model\n",
    "            \n",
    "        Args:\n",
    "            landmarks: processed keypoints from the pose estimation model\n",
    "            mp_pose: Mediapipe pose estimation model\n",
    "            side: 'left' or 'right'. Denotes the side of the body of the landmark of interest.\n",
    "            joint: 'shoulder', 'elbow', 'wrist', 'hip', 'knee', or 'ankle'. Denotes which body joint is associated with the landmark of interest.\n",
    "        \n",
    "        \"\"\"\n",
    "        coord = getattr(mp_pose.PoseLandmark,side.upper()+\"_\"+joint.upper())\n",
    "        x_coord_val = landmarks[coord.value].x\n",
    "        y_coord_val = landmarks[coord.value].y\n",
    "        return [x_coord_val, y_coord_val] \n",
    "    \n",
    "    def viz_joint_angle(self, image, angle, joint):\n",
    "        \"\"\"\n",
    "        Displays the joint angle value near the joint within the image frame\n",
    "        \n",
    "        \"\"\"\n",
    "        cv2.putText(image, str(int(angle)), \n",
    "                    tuple(np.multiply(joint, [640, 480]).astype(int)), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                            )\n",
    "        return\n",
    "\n",
    "    def count_reps(self, image, landmarks, mp_pose):\n",
    "        \"\"\"\n",
    "        Counts repetitions of each exercise. Global count and stage (i.e., state) variables are updated within this function.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.current_action == 'curl':\n",
    "            # Get coords\n",
    "            shoulder = self.get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "            elbow = self.get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "            wrist = self.get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "            \n",
    "            # calculate elbow angle\n",
    "            angle = self.calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # curl counter logic\n",
    "            if angle < 30:\n",
    "                self.curl_stage = \"up\" \n",
    "            if angle > 140 and self.curl_stage =='up':\n",
    "                self.curl_stage=\"down\"  \n",
    "                self.curl_counter +=1\n",
    "            self.press_stage = None\n",
    "            self.squat_stage = None\n",
    "                \n",
    "            # Viz joint angle\n",
    "            self.viz_joint_angle(image, angle, elbow)\n",
    "            \n",
    "        elif self.current_action == 'press':           \n",
    "            # Get coords\n",
    "            shoulder = self.get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "            elbow = self.get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "            wrist = self.get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "\n",
    "            # Calculate elbow angle\n",
    "            elbow_angle = self.calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # Compute distances between joints\n",
    "            shoulder2elbow_dist = abs(math.dist(shoulder,elbow))\n",
    "            shoulder2wrist_dist = abs(math.dist(shoulder,wrist))\n",
    "            \n",
    "            # Press counter logic\n",
    "            if (elbow_angle > 130) and (shoulder2elbow_dist < shoulder2wrist_dist):\n",
    "                self.press_stage = \"up\"\n",
    "            if (elbow_angle < 50) and (shoulder2elbow_dist > shoulder2wrist_dist) and (self.press_stage =='up'):\n",
    "                self.press_stage='down'\n",
    "                self.press_counter += 1\n",
    "            self.curl_stage = None\n",
    "            self.squat_stage = None\n",
    "                \n",
    "            # Viz joint angle\n",
    "            self.viz_joint_angle(image, elbow_angle, elbow)\n",
    "            \n",
    "        elif self.current_action == 'squat':\n",
    "            # Get coords\n",
    "            # left side\n",
    "            left_shoulder = self.get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "            left_hip = self.get_coordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "            left_knee = self.get_coordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "            left_ankle = self.get_coordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "            # right side\n",
    "            right_shoulder = self.get_coordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "            right_hip = self.get_coordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "            right_knee = self.get_coordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "            right_ankle = self.get_coordinates(landmarks, mp_pose, 'right', 'ankle')\n",
    "            \n",
    "            # Calculate knee angles\n",
    "            left_knee_angle = self.calculate_angle(left_hip, left_knee, left_ankle)\n",
    "            right_knee_angle = self.calculate_angle(right_hip, right_knee, right_ankle)\n",
    "            \n",
    "            # Calculate hip angles\n",
    "            left_hip_angle = self.calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "            right_hip_angle = self.calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "            \n",
    "            # Squat counter logic\n",
    "            thr = 165\n",
    "            if (left_knee_angle < thr) and (right_knee_angle < thr) and (left_hip_angle < thr) and (right_hip_angle < thr):\n",
    "                self.squat_stage = \"down\"\n",
    "            if (left_knee_angle > thr) and (right_knee_angle > thr) and (left_hip_angle > thr) and (right_hip_angle > thr) and (self.squat_stage =='down'):\n",
    "                self.squat_stage='up'\n",
    "                self.squat_counter += 1\n",
    "            self.curl_stage = None\n",
    "            self.press_stage = None\n",
    "                \n",
    "            # Viz joint angles\n",
    "            self.viz_joint_angle(image, left_knee_angle, left_knee)\n",
    "            self.viz_joint_angle(image, left_hip_angle, left_hip)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        return\n",
    "    \n",
    "    def prob_viz(self, res, input_frame):\n",
    "        \"\"\"\n",
    "        This function displays the model prediction probability distribution over the set of exercise classes\n",
    "        as a horizontal bar graph\n",
    "        \n",
    "        \"\"\"\n",
    "        output_frame = input_frame.copy()\n",
    "        for num, prob in enumerate(res):        \n",
    "            cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), self.colors[num], -1)\n",
    "            cv2.putText(output_frame, self.actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "        return output_frame\n",
    "\n",
    "    def process(self, image):\n",
    "        \"\"\"\n",
    "        Function to process the video frame from the user's webcam and run the fitness trainer AI\n",
    "\n",
    "        Args:\n",
    "            image (numpy array): input image from the webcam\n",
    "\n",
    "        Returns:\n",
    "            numpy array: processed image with keypoint detection and fitness activity classification visualized\n",
    "        \"\"\"\n",
    "        # Pose detection model\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        self.draw_landmarks(image, results) \n",
    "        \n",
    "        # Prediction logic\n",
    "        keypoints = self.extract_keypoints(results)        \n",
    "        self.sequence.append(keypoints.astype('float32',casting='same_kind'))      \n",
    "        self.sequence = self.sequence[-self.sequence_length:]\n",
    "        \n",
    "        if len(self.sequence) == self.sequence_length:\n",
    "            res = model.predict(np.expand_dims(self.sequence, axis=0), verbose=0)[0]\n",
    "            # interpreter.set_tensor(self.input_details[0]['index'], np.expand_dims(self.sequence, axis=0))\n",
    "            # interpreter.invoke()\n",
    "            # res = interpreter.get_tensor(self.output_details[0]['index'])\n",
    "            \n",
    "            self.current_action = self.actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "            \n",
    "            # Erase current action variable if no probability is above threshold\n",
    "            if confidence < self.threshold:\n",
    "                self.current_action = ''\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = self.prob_viz(res, image)\n",
    "            \n",
    "            # Count reps\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                self.count_reps(\n",
    "                    image, landmarks, mp_pose)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Display graphical information\n",
    "            cv2.rectangle(image, (0,0), (640, 40), self.colors[np.argmax(res)], -1)\n",
    "            cv2.putText(image, 'curl ' + str(self.curl_counter), (3,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'press ' + str(self.press_counter), (240,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'squat ' + str(self.squat_counter), (490,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "          \n",
    "        # return cv2.flip(image, 1)\n",
    "        return image\n",
    "    \n",
    "    def recv(self, frame):\n",
    "        \"\"\"\n",
    "        Receive and process video stream from webcam\n",
    "\n",
    "        Args:\n",
    "            frame: current video frame\n",
    "\n",
    "        Returns:\n",
    "            av.VideoFrame: processed video frame\n",
    "        \"\"\"\n",
    "        # img = frame.to_ndarray(format=\"bgr24\")\n",
    "        img = self.process(frame)\n",
    "        return img\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local variable 'img' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "detector = VideoProcessor()\n",
    "try:\n",
    "    while(True):\n",
    "        isTrue, img = vid.read()\n",
    "        img = cv2.flip(img,1)\n",
    "        img = detector.recv(img)\n",
    "        if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "            break\n",
    "        cv2.imshow('Video',img)\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "except Exception as e:\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
